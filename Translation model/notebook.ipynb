{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"5668eec779d44254b238b002164df808","deepnote_cell_height":153.171875,"deepnote_cell_type":"markdown","tags":[]},"source":["## Translation model"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"868def1db02b473d9d8a20da40436280","deepnote_cell_height":441,"deepnote_cell_type":"code","deepnote_output_heights":[194],"deepnote_to_be_reexecuted":false,"execution_millis":2,"execution_start":1644832740789,"source_hash":"17e55488","tags":[]},"outputs":[],"source":["from collections import Counter\n","import re\n","import os\n","\n","# Automatically go through all of the files \n","path = \"/work/dat410_europarl\"\n","file_list = os.listdir(path)\n","\n","def count_words(f, p = False):\n","    if p:\n","        print(f\"Reading words from file {f}\")\n","    words = re.findall(r'\\w+', open('/work/dat410_europarl/' + f).read())\n","    return Counter(words)\n","\n","def warmup(counter):\n","    print(\"10 most common words are:\")\n","    print(counter.most_common(10))\n","    c_total = sum(counter.values())\n","    print(f\"Probability for word beeing 'zebra': {c['zebra']/c_total}\")\n","    print(f\"Probability for word beeing 'speaker': {c['speaker']/c_total}\")\n","    print(\"------------------------------------------------------\")"]},{"cell_type":"markdown","metadata":{"cell_id":"20452a7f4f494bb4b181b98ef5da99ef","deepnote_cell_height":74.78125,"deepnote_cell_type":"markdown","tags":[]},"source":["We decided to check all documents (including all versions of the english documents) as they produce slighlty different results. Notably we can see that the probability that a word is 'speaker' is highest in the french-english document. "]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"79262cd09488487f9969a243041db784","deepnote_cell_height":773,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":912,"execution_start":1644832743467,"source_hash":"7838c87a","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading words from file europarl-v7.sv-en.lc.sv\n","10 most common words are:\n","[('att', 9181), ('och', 7038), ('i', 5954), ('det', 5687), ('som', 5028), ('för', 4959), ('av', 4013), ('är', 3840), ('en', 3724), ('vi', 3211)]\n","Probability for word beeing 'zebra': 0.0\n","Probability for word beeing 'speaker': 0.0\n","------------------------------------------------------\n","Reading words from file europarl-v7.de-en.lc.de\n","10 most common words are:\n","[('die', 10521), ('der', 9374), ('und', 7028), ('in', 4175), ('zu', 3169), ('den', 2976), ('wir', 2863), ('daß', 2738), ('ich', 2670), ('das', 2669)]\n","Probability for word beeing 'zebra': 0.0\n","Probability for word beeing 'speaker': 0.0\n","------------------------------------------------------\n","Reading words from file europarl-v7.sv-en.lc.en\n","10 most common words are:\n","[('the', 19327), ('of', 9344), ('to', 8814), ('and', 6949), ('in', 6124), ('is', 4400), ('that', 4357), ('a', 4271), ('we', 3223), ('this', 3222)]\n","Probability for word beeing 'zebra': 0.0\n","Probability for word beeing 'speaker': 3.890702388502196e-05\n","------------------------------------------------------\n","Reading words from file europarl-v7.de-en.lc.en\n","10 most common words are:\n","[('the', 19853), ('of', 9633), ('to', 9069), ('and', 7307), ('in', 6278), ('is', 4478), ('that', 4441), ('a', 4438), ('we', 3372), ('this', 3362)]\n","Probability for word beeing 'zebra': 0.0\n","Probability for word beeing 'speaker': 4.152541733044417e-05\n","------------------------------------------------------\n","Reading words from file europarl-v7.fr-en.lc.fr\n","10 most common words are:\n","[('apos', 16729), ('de', 14528), ('la', 9746), ('et', 6620), ('l', 6536), ('le', 6177), ('à', 5588), ('les', 5587), ('des', 5232), ('que', 4797)]\n","Probability for word beeing 'zebra': 0.0\n","Probability for word beeing 'speaker': 0.0\n","------------------------------------------------------\n","Reading words from file europarl-v7.fr-en.lc.en\n","10 most common words are:\n","[('the', 19627), ('of', 9534), ('to', 8992), ('and', 7214), ('in', 6197), ('is', 4453), ('that', 4421), ('a', 4388), ('we', 3341), ('this', 3332)]\n","Probability for word beeing 'zebra': 0.0\n","Probability for word beeing 'speaker': 4.570784308497469e-05\n","------------------------------------------------------\n"]}],"source":["# Warmup\n","for f in file_list:\n","    c = count_words(f, True)\n","    warmup(c)\n","    "]},{"cell_type":"markdown","metadata":{"cell_id":"c5e02c84c9174d0299dfecfd6110004d","deepnote_cell_height":54,"deepnote_cell_type":"markdown","tags":[]},"source":["#### Language modeling"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"2c11347bb4a044d9836df4e1423dc2c1","deepnote_cell_height":702.5,"deepnote_cell_type":"code","deepnote_output_heights":[155.5],"deepnote_to_be_reexecuted":false,"execution_millis":1710,"execution_start":1644832746036,"source_hash":"3cb96cf8","tags":[]},"outputs":[{"data":{"text/plain":["['jag förklarar europaparlamentets session återupptagen efter avbrottet den 17 december ',\n"," ' jag vill på nytt önska er ett gott nytt år och jag hoppas att ni haft en trevlig semester ',\n"," 'som ni kunnat konstatera ägde &quot; den stora år 2000-buggen &quot; aldrig rum ',\n"," ' däremot har invånarna i ett antal av våra medlemsländer drabbats av naturkatastrofer som verkligen varit förskräckliga ',\n"," 'ni har begärt en debatt i ämnet under sammanträdesperiodens kommande dagar ',\n"," 'till dess vill jag att vi , som ett antal kolleger begärt , håller en tyst minut för offren för bl',\n"," 'a',\n"," ' stormarna i de länder i europeiska unionen som drabbats ']"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["def read_sentences(f, add_NULL = True):\n","    sentences = re.split(r'[\\n.]',open('/work/dat410_europarl/' + f).read())\n","    while('' in sentences) :\n","        sentences.remove('')\n","    if add_NULL:\n","        for idx in range(len(sentences)):\n","            sentences[idx] = 'NULL ' + sentences[idx]\n","    return sentences\n","\n","def word_count_w_NULL(f):\n","    sentences = read_sentences(f, add_NULL=True)\n","    word_count = {}\n","\n","    for sentence in sentences:\n","        words = re.findall(r'\\w+', sentence)\n","        tmp_word_count = Counter(words)\n","        for word in tmp_word_count:\n","            if word in word_count:\n","                word_count[word]+= tmp_word_count[word]\n","            else:\n","                word_count[word] = tmp_word_count[word]\n","\n","    return word_count\n","    \n","read_sentences(file_list[0], add_NULL=False)[0:8]\n","# Note the issue while parsing the abbreviation bl.a."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"f2258cd25a1d4361af8142859438cfff","deepnote_cell_height":855,"deepnote_cell_type":"code","deepnote_output_heights":[null,21.1875],"deepnote_to_be_reexecuted":false,"execution_millis":1,"execution_start":1644832751048,"source_hash":"f74217fc","tags":[]},"outputs":[],"source":["def bigram_probs(f):\n","    # Read all words from file and store word counts\n","    word_count = word_count_w_NULL(f)\n","\n","    # Read out all sentences to prevent bigrams across sentences\n","    sentences = read_sentences(f)\n","    nmb_of_sentences = len(sentences)\n","\n","    # dictionary for storing bigram counts\n","    tot_bi_counts = {}\n","    for sentence in sentences:\n","        sentence_words = re.findall(r'\\w+', sentence)\n","        bi_count = Counter((sentence_words[idx],sentence_words[idx+1]) for idx in range(len(sentence_words) - 1))\n","\n","        # Update the total counts for the bigrams\n","        for bigram in bi_count.elements():\n","            if(bigram in tot_bi_counts):\n","                tot_bi_counts[bigram] += bi_count[bigram]\n","            else:\n","                tot_bi_counts[bigram] = bi_count[bigram]\n","    \n","    # Dictionary that stores P(word2|word1)\n","    bi_probs = {}\n","    for bigram in tot_bi_counts:\n","        bi_probs[bigram] = tot_bi_counts[bigram]/word_count[bigram[0]]\n","\n","    return bi_probs , word_count\n","\n","# Split into functions for easier debugging and improvement\n","def calculate_sentence_prob(sentence, bi_probs, word_count):\n","    words = re.findall(r'\\w+', sentence)\n","    \n","    prob = 1\n","    for idx in range(len(words)-1):\n","        try:\n","            prob *= bi_probs[(words[idx], words[idx+1])]\n","        except:\n","            # If we encounter word we haven't seen we calculate the probability \n","            # it would have if it were in the data (Not entierly accurate since we don't modify the rest of bi_probs)\n","            if(words[idx] in word_count):\n","                1/word_count[words[idx]]\n","            # Note: P(Known|Unknown) = 1/1 therefore it's just skipped (same as prob*1)\n","    return prob\n"]},{"cell_type":"markdown","metadata":{"cell_id":"9e2449c3d3f941d5b357456e489e34cd","deepnote_cell_height":111.171875,"deepnote_cell_type":"markdown","tags":[]},"source":["In our code we try to calculate the probability one instance of the unseen word would have had if it were part of the dataset. It's not entierly accurate since we do not recalculate what the other items probabilities would have had but this seems reasonable since we can't always trust the input text to this function. \n","\n","As we can see the probabilities are quite low and gets significantly lower the longer the sentences gets."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"5c7ff60baea04727808e3b58214ad034","deepnote_cell_height":321.125,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":3855,"execution_start":1644832763523,"source_hash":"a25e4cc0","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Sentence from source text....\n","6.955939123260558e-18\n","Sentence with unknown words...\n","5.381281887355061e-12\n","Long sentence... \n","4.198484462055951e-111\n"]}],"source":["bp, word_count = bigram_probs(file_list[0])\n","print('Sentence from source text....')\n","print(calculate_sentence_prob(\"alla gläder vi oss åt att domstolen har friat honom\", bp, word_count)) \n","print('Sentence with unknown words...')\n","print(calculate_sentence_prob(\"Jag gillar vill gå på bio och titta på spiderman på fredag\", bp, word_count)) \n","print('Long sentence... ')\n","print(calculate_sentence_prob(\"på uppmaning av en fransk parlamentsledamot , zimeray , har redan en framställning gjorts , undertecknad av många , bland annat jag själv , men jag uppmanar er , i enlighet med de riktlinjer som europaparlamentet och hela den europeiska gemenskapen alltid har hållit fast vid , att med all den tyngd ni har i kraft av ert ämbete och den institution ni företräder , uppmana texas guvernör , bush , att uppskjuta verkställigheten och att benåda den dömde\", bp, word_count)) "]},{"cell_type":"markdown","metadata":{"cell_id":"b94c2f8127904fa4893948a3211677c7","deepnote_cell_height":54,"deepnote_cell_type":"markdown","tags":[]},"source":["#### Translation modeling"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"4ddc3694750d4e27867dcc73b9af9fc8","deepnote_cell_height":1143,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":153072,"execution_start":1644832768490,"is_output_hidden":false,"source_hash":"133db30d","tags":[]},"outputs":[],"source":["import random\n","\n","def IBMOne(scr_file, en_file, iterations = 10):\n","    # Extract lines from files\n","    scr_lines = open('/work/dat410_europarl/' + scr_file).read().splitlines()\n","    en_lines = open('/work/dat410_europarl/' + en_file).read().splitlines()\n","\n","    # Initilize required dicts\n","    scr_given_en = init_word_translation_probs(scr_lines, en_lines)\n","    en_word_count = word_count_w_NULL(en_file)\n","    scr_en_pair_count = init_word_translation_probs(scr_lines, en_lines)\n","\n","    for _ in range (iterations):\n","        # Set all counts to 0\n","        for word in en_word_count:\n","            en_word_count[word] = 0\n","        for pair in scr_en_pair_count:\n","            scr_en_pair_count[pair] = 0\n","\n","        # Algorithm\n","        for line in range(len(scr_lines)):\n","            scr_words = re.findall(r'\\w+', scr_lines[line])\n","            en_words = ['NULL'] + re.findall(r'\\w+', en_lines[line])\n","\n","            for scr_word in scr_words:\n","                p_sum = 0\n","                for en_word in en_words:\n","                    p_sum += scr_given_en[(scr_word,en_word)]\n","\n","                for en_word in en_words:\n","                    #compute alignment prob 0.1 / 0.95\n","                    prob = scr_given_en[(scr_word,en_word)] / p_sum\n","                    #update pseudocount\n","                    scr_en_pair_count[(scr_word,en_word)] += prob\n","                    #update pseudocount\n","                    en_word_count[en_word] += prob\n","\n","        #reestimate probabilities\n","        for word_pair in scr_given_en:\n","            scr_given_en[word_pair] = scr_en_pair_count[word_pair] / en_word_count[word_pair[1]]\n","\n","    return scr_given_en #This is what we later call \"dictionary\" during the tranlation part\n","\n","\n","# Initialization dict with all word pairs that can be creating by matchining scr<->en lines\n","def init_word_translation_probs(scr_lines,en_lines, zeroes = False):\n","    translation_probs = dict()\n","\n","    for line in range(len(scr_lines)):\n","        scr_words = re.findall(r'\\w+', scr_lines[line])\n","        en_words = ['NULL'] + re.findall(r'\\w+', en_lines[line])\n","\n","        for scr_word in scr_words:\n","            for en_word in en_words:\n","                # Assign each pair a random number\n","                translation_probs[(scr_word,en_word)] = random.uniform(0, 0.99)\n","\n","    return translation_probs\n","\n","svGivenEn= IBMOne(file_list[0], file_list[2])"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"303b7a503fef42fbb99165742e8f1e46","deepnote_cell_height":398.875,"deepnote_cell_type":"code","deepnote_output_heights":[193.875],"deepnote_to_be_reexecuted":false,"execution_millis":181,"execution_start":1644833028673,"source_hash":"85ed0242","tags":[]},"outputs":[{"data":{"text/plain":["[(0.8539941237408167, 'europeiska'),\n"," (0.08068846303480778, 'europeisk'),\n"," (0.01621272820284629, 'i'),\n"," (0.007984925393355307, 'till'),\n"," (0.006425479935892108, 'en'),\n"," (0.005689820584388745, 'för'),\n"," (0.003993640802996053, 'den'),\n"," (0.0037668360247995096, 'unionen'),\n"," (0.0036315362233786206, 'europaparlamentet'),\n"," (0.0032533871471700777, 'om')]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# 10 most probable words that translate to \"european\" from swedish\n","european_in_sv = []\n","for elem in svGivenEn:\n","    if(elem[1] == \"european\"):\n","        european_in_sv.append((svGivenEn[elem], elem[0]))\n","european_in_sv.sort(reverse=True)\n","european_in_sv[0:10]"]},{"cell_type":"markdown","metadata":{"cell_id":"5c030b0c5c244e2abffa2ece7d7d4c41","deepnote_cell_height":54,"deepnote_cell_type":"markdown","tags":[]},"source":["#### Decoding"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"97f2c6f9fa72494c84c7de92a7118e72","deepnote_cell_height":1755,"deepnote_cell_type":"code","deepnote_output_heights":[1],"deepnote_to_be_reexecuted":false,"execution_millis":1,"execution_start":1644834138737,"source_hash":"48788a3c","tags":[]},"outputs":[],"source":["import itertools\n","import numpy as np\n","\n","class Translator():\n","    def __init__(self, scr_file, en_file, iterations=10):\n","        self.bp, self.en_word_count = bigram_probs(en_file)\n","        self.dictionary = IBMOne(scr_file, en_file, iterations)\n","\n","    def translate(self, sentence):\n","        words = re.findall(r'\\w+', sentence)\n","        translated_words = []\n","        for word in words:\n","            translated_word = self.most_prob_word(word)\n","            translated_words.append(translated_word)\n","\n","        #order with sentence probability model model (maximize)\n","        translated_sentence = self.calculate_sentence_prob_maximize_two(translated_words)\n","        return translated_sentence\n","\n","    def most_prob_word(self, scr_word):\n","        highest_prob = 0\n","        most_prob_word = \"\"\n","        # translation is a word pair in our dictionary (scr_word, en_word)\n","        for translation in self.dictionary:\n","            if translation[0] == scr_word:\n","                if self.dictionary[translation] > highest_prob:\n","                    most_prob_word = translation[1]\n","                    # Reads out the stored probability of that translation from dictionary\n","                    highest_prob = self.dictionary[translation]\n","        if most_prob_word == \"\":\n","            # If word is not in our dictionary\n","            most_prob_word = scr_word\n","        return most_prob_word\n","\n","    # tries all ordered combinations of words in tranlated word list and returns the most likely ordering of them \n","    # using P(E) function. An optimal layout would also try different tenses of the words \n","    # and also try with different filler words, but for this assignment we have implemented the simplest model \n","    def calculate_sentence_prob_maximize(self, word_list): \n","        res_sentence = \"\"\n","        highest_prob = 0\n","\n","        for possible_sentence in map(\" \".join, itertools.permutations(word_list)):\n","            sentence_prob = calculate_sentence_prob(possible_sentence, self.bp, self.en_word_count)\n","            if  sentence_prob > highest_prob:\n","                res_sentence = possible_sentence\n","                highest_prob = sentence_prob\n","            \n","            while 'NULL' in res_sentence:\n","                res_sentence.remove('NULL')\n","\n","        return res_sentence\n","\n","    # Looks only at the most probable word of translated words to come after the previous word according to bi_probs model.\n","    def calculate_sentence_prob_maximize_two(self, word_list): \n","        res_sentence = \"\"\n","        last_word = \"\"\n","        maybe_word = \"\"\n","        highest_prob = 0\n","        #start word\n","        for word in word_list:\n","            try:\n","                if self.bp[('NULL',word)] > highest_prob:\n","                    maybe_word = word\n","                    highest_prob = self.bp[('NULL',word)]\n","            except:\n","                if maybe_word == \"\":\n","                    maybe_word = word\n","        word_list.remove(maybe_word)\n","        res_sentence += maybe_word + \" \"\n","        last_word = maybe_word\n","        maybe_word = \"\"\n","        highest_prob = 0\n","\n","        # rest of the words\n","        while len(word_list) > 0:\n","            for word in word_list:\n","                try:\n","                    if self.bp[(last_word, word)] > highest_prob:\n","                        maybe_word = word\n","                        highest_prob = self.bp[(last_word,word)]\n","                except:\n","                    if maybe_word == \"\":\n","                        maybe_word = word\n","            word_list.remove(maybe_word)\n","            res_sentence += maybe_word + \" \"\n","            last_word = maybe_word\n","            maybe_word = \"\"\n","            highest_prob = 0\n","        \n","        while 'NULL' in res_sentence:\n","            res_sentence.remove('NULL')\n","\n","        return res_sentence\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"3ef121ac9b4e46af922e0146dc703db2","deepnote_cell_height":81,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":150909,"execution_start":1644834139137,"source_hash":"d17b8887","tags":[]},"outputs":[],"source":["sv_to_en = Translator(file_list[0], file_list[2])"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"5634b36a179f44e5b1c891b055bd9256","deepnote_cell_height":136.1875,"deepnote_cell_type":"code","deepnote_output_heights":[21.1875],"deepnote_to_be_reexecuted":false,"execution_millis":249,"execution_start":1644834290125,"source_hash":"a72b2be6","tags":[]},"outputs":[{"data":{"text/plain":["'we must agreeing '"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["# Short test sentence\n","sv_to_en.translate(\"vi måste enas\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"b25d80e7dee548d1ae2e7c26eb3a4aaa","deepnote_cell_height":136.1875,"deepnote_cell_type":"code","deepnote_output_heights":[21.1875],"deepnote_to_be_reexecuted":false,"execution_millis":799,"execution_start":1644834290406,"source_hash":"413f2290","tags":[]},"outputs":[{"data":{"text/plain":["'for not for question currently presently anyone request '"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["# Sentence from text\n","sv_to_en.translate(\"frågan för närvarande inte föremål för någon begäran\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"d67ede68fe854776a7b43ab73456ee20","deepnote_cell_height":81,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2,"execution_start":1644780736163,"source_hash":"b623e53d","tags":[]},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown","tags":[]},"source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=4ba8ee87-29e0-4e7b-bdf0-521dbe79e112' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"]}],"metadata":{"deepnote":{"is_reactive":false},"deepnote_execution_queue":[],"deepnote_notebook_id":"feb8b77cadf845b6893d89f4134eb365","kernelspec":{"display_name":"Python 3.9.0 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.0"},"orig_nbformat":2,"vscode":{"interpreter":{"hash":"1df5a56217a3f66810d59d874eef91129c6890896cf0a2944b6c9cb196ed5a93"}}},"nbformat":4,"nbformat_minor":0}
